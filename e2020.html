<!DOCTYPE html>
<html>
<head>
	<title>2020 election prediction</title>
</head>
<body>
	<p>For the 2020 Democratic primaries, I have decided to predict results using a simple regression based on demographics. To accomplish this, I performed the following steps:</p>
	<ul>
		<li>Download data tables full of demographic estimates (specifically, the most recent edition of the American Community Survey) from the <a href="https://data.census.gov/cedsci/">United States Census Bureau</a>. I did this for counties (to generate county-level result maps) and congressional districts (to predict delegate totals, as delegates are awarded according to results in each congressional district of a state).</li>
		<li>Use principal component analysis to distill the data down to its most important features.</li>
		<li>Obtain results from the primaries/caucuses that have already happened (from <a href="https://uselectionatlas.org">Dave Leip's election atlas</a>) to use as fitting data. I normalized the results for each candidate in a given state by that candidate's statewide average, in order to remove the confounding factor of changing poll numbers and get results that would just reflect demographic strengths and weaknesses.</li>
		<li>For each candidate, do a quadratic multivariate regression utilizing the most important principal components (in my case the first six, which together explained about 96% of the variance). This generates a surface representing that candidate's support as a function of demographics. I used <a href="http://www.ahmetcecen.tech/MultiPolyRegress-MatlabCentral/">MATLAB code created by Ahmet Cecen</a> for this.</li>
		<li>Use these surfaces to estimate support for each candidate relative to their statewide support in each county or congressional district being predicted. Then, multiply by each candidate's current polling numbers and scale to 100% to get raw support in percentage points.</li>
		<li>After Super Tuesday, the two remaining candidates with the most support were Bernie Sanders and Joe Biden. Therefore, instead of fitting support surfaces for each candidate, I decided to perform regression on the ratio of support for Sanders to support for Biden. This made it easier to make predictions of county winners and earned delegates.</li>
	</ul>
	<p>To test my model, I decided to predict the results in South Carolina using those from Iowa, New Hampshire and Nevada.</p>
	<p><input type="button" value="Show the South Carolina predictions"></p>
	<div id="sc_predictions" style="display:none">
		The model successfully predicted that Biden would outperform his polls, although it allotted him fewer delegates than he actually received due to having Steyer above the threshold to receive delegates. The model also missed on some county-level predictions, although it performed rather well in terms of predicting which counties each candidate would be strong in. I viewed this test of the model as a success, particularly because the three states that voted previously (IA, NH and NV) are demographically dissimilar to South Carolina, making a prediction based on regression difficult. The model's predictions were as follows:
		<ul>
			<li>Joe Biden: 27 delegates, wins in all counties not mentioned below</li>
			<li>Bernie Sanders: 15 delegates, wins in Aiken, Anderson, Georgetown, Greenville, Horry, Lancaster, Oconee, Pickens and Spartanburg Counties</li>
			<li>Tom Steyer: 12 delegates, wins in Abbeville, Berkeley, Cherokee, Dorchester, Greenwood, Lexington and York Counties</li>
			<li>All other candidates: 0 delegates, no county wins</li>
		</ul>
		<img src="sc_predictions.png" alt="South Carolina model predictions" style="width:500px; height:398.5px;">
	</div>
	<p>Prior to Super Tuesday, I made predictions and performed analysis on the model output. This included interpretations of patterns in candidate support as well as of my principal components.</p>
	<p><input type="button" value="Show Super Tuesday predictions and analysis"></p>
	<div id="super_tuesday_analysis" style="display:none">
		<p>Before I introduce my predictions, two important notes are necessary. Firstly, my model is only as good as its input. After performing regression to get candidate support as a function of demographics, normalized to a hypothetical baseline of support, I input the then-current polling numbers to get actual numerical predictions. Therefore, since the polls were off due to not having caught up with fast-moving events, so was my model. Secondly, both my predictions and my commentary are solely based on model output. I'm not attempting to put my thumb on the scale, and I'm definitely not attempting to inject personal views into any of this. I'm a mathematician, not a pundit. Having said that, I have included a county map of my Super Tuesday predictions below. Green is Sanders, blue is Biden, red is Warren and magenta is Bloomberg.</p>
		<p><img src="county_predictions_supertuesday.svg" alt="Super Tuesday county-level predictions"></p>
		<p>I'll also mention my delegate predictions before I get into the math:</p>
		<ul>
			<li>Alabama: Biden 30, Sanders 11, Bloomberg 11</li>
			<li>Arkansas: Biden 12, Sanders 9, Bloomberg 9, Warren 1</li>
			<li>Massachusetts: Sanders 37, Warren 32, Biden 18, Bloomberg 4</li>
			<li>Maine: Sanders 9, Bloomberg 6, Biden 5, Warren 4</li>
			<li>North Carolina: Biden 47, Sanders 35, Bloomberg 23, Warren 5</li>
			<li>Oklahoma: Biden 13, Sanders 12, Bloomberg 8, Warren 4</li>
			<li>Tennessee: Biden 25, Sanders 23, Bloomberg 15, Warren 1</li>
			<li>Vermont: Sanders 12, Warren 4</li>
			<li>Virginia: Biden 56, Sanders 22, Bloomberg 17, Warren 4</li>
			<li>In the other states, I wasn't able to finish calculating delegate apportionment before polls closed.</li>
		</ul>
		<p>Since Bloomberg did not campaign in any of the states prior to Super Tuesday, my model treated him as a blank slate in terms of demographic patterns of support. This means that was predicted to do best in areas that didn't particularly fit in with the other major candidates' bases. On the map above, Bloomberg's county wins are in Mormon-heavy counties in rural Utah (where he is helped by a high polling average as of March 3), ski resorts in Colorado, Native American areas in Oklahoma and Minnesota, and a county in Texas (Coryell) that contains a large military population. All of these have their own statistical signatures in their demographics that for whatever reason didn't match with high levels of support for Sanders, Biden or Warren. In actuality, Bloomberg's support correlated quite well with the third principal component I used (see below). This meant that he did in fact do well in the ski counties, and he did better in rural compared to urban Utah, but the rest of the demographic outliers assigned to him didn't follow suit.</p>
		<p>My model successfully predicted Biden's two main sources of support on Super Tuesday, namely counties with high values of principal components 2 and 3. I will visualize this, starting with an explanation of my principal components. When the first two principal components that I base my model on are plotted against each other for all 3142 counties in the United States, the result is a rather tidy-looking triangle:</p>
		<p><img src="pc_1_2.svg" alt="First two principal components"></p>
		<p>The vertices of this triangle are Garfield County, Montana (high PC1), Jefferson County, Mississippi (low PC1, high PC2), and Starr County, Texas (low PC1, low PC2). In some ways, these represent the Platonic ideals of rural counties in the Midwest/northern Great Plains, Southeast, and Southwest, respectively. Distance from the second of these vertices correlates well with percent black population in a county.</p>
		<p>Socioeconomic status is often a nebulous concept, typically based around income and education, and as a result it is hard to quantify. However, my third principal component seems to correlate very well with it. This can be attested to by a map of PC3 on a county level (below). Here, counties are shaded according to which quintile for PC3 they belong to (with red being low and green being high); additionally, the top 5% of counties are highlighted by being in a darker green than the rest of the top quintile. The highest counties are rich and well-educated, while the lowest are poor and mostly rural:</p>
		<p><img src="pc3_quintiles_top5.svg" alt="Principal component 3 by county, sorted into quintiles with top 5% highlighted"></p>
		<p>Plotting Biden's results as a function of PC2 and PC3 demonstrates his two main bases (high black population and high socioeconomic status). This shows up in the prediction map above, as well as in the actual results. For example, both the model and the eventual results had Biden sweeping every county in Alabama, and doing best in East Texas and West Tennessee. Additionally, the model correctly determined that Douglas County would be Biden's best in Colorado (disregarding a couple small rural counties with fewer than 100 votes cast each), and that he would do well in the Bay Area. The relationship that his support has with both PC2 and PC3 can be seen in this figure I created prior to making Super Tuesday predictions:</p>
		<p><img src="proj_biden_supertuesday_pc23.svg" alt="Projected support for Biden as a function of principal components 2 and 3"></p>
		<p>My model predicted support for Sanders fairly well, although it had room for improvement. Since the counties in the four pre-Super Tuesday states didn't cover the entire principal component space, part of the model's predictions were necessarily extrapolations. The model had Sanders doing extremely well in areas with low values of both PC1 and PC2 (anecdotally, these areas were generally rural and low-density with high Hispanic populations, with many in Texas). While these areas did give Sanders above average support, they were far from the predicted blowout wins. Prior to Super Tuesday, the model also found support for Sanders to be negatively correlated with PC3, which can be seen in this graph which plots PC1 and PC3:</p>
		<p><img src="proj_sanders_supertuesday_pc13.svg" alt="Projected support for Sanders as a function of principal components 1 and 3"></p>
		<p>This led to counterintuitive predictions such as Sanders doing very well in rural West Texas but poorly in Austin. It also led to an overstatement of his performance in rural Tennessee (although the model correctly viewed the mountains of East Tennessee as potentially his best area, as it did with similar areas in western North Carolina). The actual Super Tuesday results instead suggest that Sanders's support is less related to PC3 than the interface of PC3 and PC1, since many low PC3/high PC1 areas predicted for Sanders actually went to Biden, but some high PC3/low PC1 areas (Austin being one example) did the opposite.</p>
		<p>The model identified Warren's bases of support almost perfectly. These were areas high in both PC1 and PC3 and low in PC6; most of the areas with the lowest values for PC6 contained college towns. For instance, in Minnesota, her best counties both in the model and reality were the suburban ones in the Twin Cities metro area (e.g. Dakota, Scott, Washington), while in Virginia she did relatively well in NoVa. The model also correctly predicted that her best county in North Carolina would be Orange. The correlation coefficients between predicted support for Warren in Super Tuesday counties and PC1 was 0.53, while that between Warren's predicted support and PC3 was 0.42. These were the two strongest correlations between support for any candidate and any of my first six principal components, and largely borne out by the results. Plotting Warren's predicted support as a function of PC1 and PC3 demonstrates this, as well as the differences between her support and Sanders's:</p>
		<p><img src="proj_warren_supertuesday_pc13.svg" alt="Projected support for Warren as a function of principal components 1 and 3"></p>
	</div>
	<p>My model did very well predicting the results of the March 10 primaries, as the Super Tuesday results gave it a lot of data to work with from all areas of my principal component space. Starting with March 10, I only used national polling (and regression based on demographics, as before) to make my predictions, due to the unreliability of state-level polling on Super Tuesday and the general lack of polls in some states holding elections on March 10.</p>
	<p><input type="button" value="Show March 10 predictions"></p>
	<div id="march_10" style="display:none">
		<p>Using the Super Tuesday results as training data, I predicted the following delegate allocations for the March 10 states on March 10 at 7:14 PM (before polls closed anywhere):</p>
		<ul>
			<li>Idaho: Sanders 11, Biden 9</li>
			<li>Michigan: Biden 80, Sanders 45</li>
			<li>Mississippi: Biden 30, Sanders 5</li>
			<li>Missouri: Biden 44, Sanders 24</li>
			<li>North Dakota: Sanders 8, Biden 6</li>
			<li>Washington: Sanders 46, Biden 43</li>
		</ul>
		<p>A county-level map of my predictions is below. A few observations about the model output:</p>
		<ul>
			<li>In Michigan, the model correctly predicted the areas of best support for Sanders (college towns, as well as some but not all mid-sized cities). However, the model predicted Wayne County as going to Biden in a landslide, when in reality it voted at approximately the statewide average. This overestimation of Biden in Wayne County caused the model to overestimate Sanders everywhere else in the state, although (as mentioned previously) the model got each candidate's relative patterns of support correct. The counties that Sanders came closest to winning in real life (Houghton, Marquette, Grand Traverse, Isabella, Kent, Ingham, Kalamazoo, Washtenaw) almost exactly matched Sanders's best counties in the model (Sanders was predicted to win 48% in Kent County). Additionally, Sanders performed a few points better in rural areas in the northwestern part of the Lower Peninsula around Grand Traverse compared to rural counties elsewhere in the state, which was successfully picked up by the model. Overall, the real life delegate results were slightly better for Sanders than the model predictions, although the model correctly predicted the delegate breakdowns in most of the congressional districts (for instance, that the 5th district would be very good for Biden).</li>
			<li>In Missouri, the model exactly predicted the number of delegates awarded to Biden and Sanders (44 and 24), although there was some deviation in the district-level results. As with Wayne County, Michigan, the model predicted Missouri's 1st congressional district to be a Biden landslide (when in reality it was less so) and boosted its prediction for Sanders across the rest of the state to compensate. However, as in Michigan, the model got the relative support levels for each candidate mostly correct. For instance, the model determined that Sanders would be strong in urban Southwest Missouri, as well as in Boone County and to a lesser extent Johnson County. While Biden won every county in Missouri, these predictions do match the relative patterns of support for the candidates.</li>
			<li>The model predicted that Biden would win Mississippi in a complete landslide, which matched the real-life results. It also was correct in picking out Hancock and Lafayette as Sanders's two best counties, and the 4th as his best congressional district. The model predicted Sanders to win 5 delegates rather than the 2 that he actually received; this is because it predicted Sanders to narrowly pass the 15% delegate threshold statewide, while in reality he was narrowly below it with 14.8% of the vote.</li>
			<li>The model correctly saw Washington as being very close (in reality, the winner in Washington was not called until a few days after election night), but thought Sanders would narrowly win rather than Biden doing so. This was because Washington is demographically similar to places like Colorado and Utah that Sanders had done well in previously. Nevertheless, the model was correct in terms of the candidates' predicted support patterns. For instance, it determined that Sanders would generally do better east of the Cascades, particularly in rural Hispanic areas, which matched the real-life results. Sanders's two best counties, namely Whitman (home to WSU) and Whatcom (home to WWU), were the same in the model and in real life. The model also correctly had Sanders do well in the urban 7th congressional district but poorly in the suburban 1st and 8th districts (all three are upscale, but in different ways, which my principal component analysis picked up). Hence, the prediction that Biden would win King County off the back of strong performance in the 1st and 8th districts was also proved right. (The 7th congressional district was also very strong for Warren due to mail-in ballots for her from prior to when she dropped out; that district has one of the highest values of PC3 of any district in the country, which was found to heavily correlate with support for Warren while she was in the race.) Around Puget Sound, the counties that Sanders did best in (San Juan, Jefferson, Thurston, and the aforementioned Whatcom) were all predicted by the model as being among his best areas; anecdotally, these areas all have strong artistic/countercultural elements.</li>
			<li>Idaho was predicted by the model to be reasonably good for Sanders, but Biden won it in the end. As with Washington, this was due to Idaho being statistically close to areas where Sanders did well previously. Sanders's best two counties, Latah and Teton, were called by the model; he also came within a few points of winning many of the other counties that the model had him doing well in (specifically Ada, Bannock, Bingham, Bonneville and Madison). Although Idaho was the model's worst-performing state on March 10, it still predicted most of the candidates' areas of relative strength and wasn't too far off the delegate distribution, despite having no state-level polling to use as an indicator.</li>
			<li>While North Dakota had a primary election on March 10, it did not report votes by county, so I cannot compare the model output to any final results. However, the model did get the delegate allocation (Sanders 8, Biden 6) exactly right.</li>
		</ul>
		<img src="county_predictions_mar10.svg" alt="March 10 county-level predictions">
	</div>
	<p>On March 17, I trained the model on results from Super Tuesday and March 10, as well as national polling trends, in order to make predictions. You can see these below:</p>
	<p><input type="button" value="Show March 17 predictions"></p>
	<div id="march_17" style="display:none">
		<p>At 6:39 PM EDT, before polls closed anywhere, I made predictions for Arizona, Florida and Illinois. I did not make any for Ohio, since voting there was postponed due to COVID-19. Since the model had Biden winning all but two of the 184 counties voting on March 17, the map below also shows the predicted percentage support for each candidate. In the two green counties, Sanders is projected to win with 50-60 percent of the vote. The blue ones represent wins for Biden, with the shade of blue used indicating Biden's margin of victory. In counties with the lightest shade of blue, Biden is projected to win the county with 50-60%, and hence Sanders is expected to clear 40%; darker shades of blue similarly represent higher deciles of support for Biden and lower ones for Sanders. On a statewide level, the model likes Arizona for Sanders owing to its demographic similarity to some of his best areas among those that have already voted (although Biden is still the clear favourite there). Both Florida and Illinois are predicted to go for Biden by approximately a 2-1 margin. The delegate breakdown by state and a county-level map of predictions are below:</p>
		<ul>
			<li>Arizona: Biden 38 delegates, Sanders 29. Best congressional districts for Biden: 4th and 8th. Best for Sanders: 9th and 1st.</li>
			<li>Florida: Biden 149 delegates, Sanders 70. Best districts for Biden: 5th, 20th, 24th and 2nd. Best for Sanders: 27th, 25th, 7th and 26th.</li>
			<li>Illinois: Biden 108 delegates, Sanders 47. Best districts for Biden: 1st, 7th and 2nd. Best for Sanders: 4th, 5th and 8th.</li>
		</ul>
		<img src="county_predictions_mar17.svg" alt="March 17 county-level predictions">
	</div>
	<script type="text/javascript">
		var scButton = document.querySelectorAll('input[type="button"]')[0];
		var analysisButton = document.querySelectorAll('input[type="button"]')[1];
		var mar10Button = document.querySelectorAll('input[type="button"]')[2];
		var mar17Button = document.querySelectorAll('input[type="button"]')[3];

		scButton.onclick = function() {
			if (scButton.value === "Show the South Carolina predictions") {
				document.getElementById("sc_predictions").style.display = "block";
				scButton.value = "Hide the South Carolina predictions";
			}
			else {
				document.getElementById("sc_predictions").style.display = "none";
				scButton.value = "Show the South Carolina predictions";
			}
		}

		analysisButton.onclick = function() {
			if (analysisButton.value === "Show Super Tuesday predictions and analysis") {
				document.getElementById("super_tuesday_analysis").style.display = "block";
				analysisButton.value = "Hide Super Tuesday predictions and analysis";
			}
			else {
				document.getElementById("super_tuesday_analysis").style.display = "none";
				analysisButton.value = "Show Super Tuesday predictions and analysis";
			}
		}

		mar10Button.onclick = function() {
			if (mar10Button.value === "Show March 10 predictions") {
				document.getElementById("march_10").style.display = "block";
				mar10Button.value = "Hide March 10 predictions";
			}
			else {
				document.getElementById("march_10").style.display = "none";
				mar10Button.value = "Show March 10 predictions";
			}
		}

		mar17Button.onclick = function() {
			if (mar10Button.value === "Show March 17 predictions") {
				document.getElementById("march_17").style.display = "block";
				mar10Button.value = "Hide March 17 predictions";
			}
			else {
				document.getElementById("march_17").style.display = "none";
				mar10Button.value = "Show March 17 predictions";
			}
		}
	</script>
</body>
</html>
